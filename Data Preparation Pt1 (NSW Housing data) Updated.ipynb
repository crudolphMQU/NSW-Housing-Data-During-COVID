{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d05584c",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Preparation-Part-1---NSW-Housing-Data\" data-toc-modified-id=\"Data-Preparation-Part-1---NSW-Housing-Data-1\">Data Preparation Part 1 - NSW Housing Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#STEP1.-Clean-Individual-Sales-and-Rent-File\" data-toc-modified-id=\"STEP1.-Clean-Individual-Sales-and-Rent-File-1.1\">STEP1. Clean Individual Sales and Rent File</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Sales-Data\" data-toc-modified-id=\"1.1-Sales-Data-1.1.1\">1.1 Sales Data</a></span></li><li><span><a href=\"#1.2-Rent-Data\" data-toc-modified-id=\"1.2-Rent-Data-1.1.2\">1.2 Rent Data</a></span></li></ul></li><li><span><a href=\"#STEP2.-Merge-Sales-Files\" data-toc-modified-id=\"STEP2.-Merge-Sales-Files-1.2\">STEP2. Merge Sales Files</a></span></li><li><span><a href=\"#STEP3.-Merge-Rent-Files\" data-toc-modified-id=\"STEP3.-Merge-Rent-Files-1.3\">STEP3. Merge Rent Files</a></span></li><li><span><a href=\"#STEP4.-Create-Master-Stacked-Sales-&amp;-Rent-File\" data-toc-modified-id=\"STEP4.-Create-Master-Stacked-Sales-&amp;-Rent-File-1.4\">STEP4. Create Master Stacked Sales &amp; Rent File</a></span></li><li><span><a href=\"#STEP5:-Create-Master-Unstacked-Sales-&amp;-Rent-File\" data-toc-modified-id=\"STEP5:-Create-Master-Unstacked-Sales-&amp;-Rent-File-1.5\">STEP5: Create Master Unstacked Sales &amp; Rent File</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdbea9",
   "metadata": {},
   "source": [
    "# Data Preparation Part 1 - NSW Housing Data\n",
    "\n",
    "The purpose of this notebook is to:\n",
    "1. Clean each individual quarterly sales file and rent files from Q3 2017 to Q1 2021 before merging all sales and rent data together.\n",
    "\n",
    "\n",
    "2. Merge the cleaned sales files and the rent files into one complete housing dataset. This file will be in the **'stacked'** format, i.e., one quarter of data is stacked on top of another quarter's with multiple entry of the same postcodes in the index, and be used mainly for exploring the trends in the housing market.\n",
    "\n",
    "\n",
    "3. Create the **unstacked** dataframe of the lastest 5 quarters (Q1 2020 - Q1 2021) of data for modelling, which will have the housing statistics from previous quarters on columns to be used as input variables together with demographic and economic features.\n",
    "\n",
    "The end result of this notebook is two .csv data files:\n",
    "* <i>Master_Sales_Rent_2017Q4_2021Q1.csv</i> (Stacked complete housing data Q4'17-Q1'21)\n",
    "* <i>Pivot_Sales_Rent_5Quarters_SharedPOA.csv</i> (Unstacked complete housing data Q1'20-Q1'21)\n",
    "\n",
    "Both are saved under `Files/Cleaned`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9619b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import glob\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77190eb9",
   "metadata": {},
   "source": [
    "## STEP1. Clean Individual Sales and Rent File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146648ed",
   "metadata": {},
   "source": [
    "### 1.1 Sales Data\n",
    "\n",
    "Define `salesCleanFn` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salesCleanFn(dataFolderString,osString):\n",
    "    # Getting all the file names from a specified folder.\n",
    "    \n",
    "    #__________________________________________________________\n",
    "    # If operating system is windows\n",
    "    if osString == 'windows':\n",
    "        dataDir = \"Files\\\\\"\n",
    "        if dataFolderString[-1] != '\\\\':\n",
    "        #if the dataFolderString does not have a forward slash, add a forward slash to the string\n",
    "            fileNames = glob.glob(dataDir+dataFolderString+'\\\\'+'*.xlsx')\n",
    "        else:\n",
    "            fileNames = glob.glob(dataDir+dataFolderString+'*.xlsx')\n",
    "\n",
    "    #__________________________________________________________\n",
    "    # If operating system is mac / linux \n",
    "    else:\n",
    "        dataDir = \"Files/\"\n",
    "        if dataFolderString[-1] != '/':\n",
    "            fileNames = glob.glob(dataDir+dataFolderString+'/'+'*.xlsx')\n",
    "        else:\n",
    "            fileNames = glob.glob(dataDir+dataFolderString+'*.xlsx')\n",
    "#     print(fileNames)\n",
    "    \n",
    "    #__________________________________________________________\n",
    "    # Looping through the fileNames to read the excel sheets. \n",
    "    frames = []\n",
    "    masterDF = []\n",
    "\n",
    "    for i, fileString in enumerate(fileNames):\n",
    "        for j in range(0,8):\n",
    "            df = []\n",
    "            df = pd.read_excel(fileString, sheet_name=\"Postcode\", na_values='-', header=j)\n",
    "\n",
    "            if df.columns[0] == 'Postcode': #  checking if the 'header' parameter has the correct j value...\n",
    "\n",
    "                # _____________________________________________________________________________________\n",
    "                # adding additional columns\n",
    "                regex = re.compile(r'\\d+') #finds all numbers in string\n",
    "                fileNumbers = regex.findall(fileString) #only works if the format of the \n",
    "                # filename is consistent. Stores the 'regex' numbers in a list.\n",
    "                \n",
    "                df['key'] = 's'+fileNumbers[2] # fileNumbers type = string\n",
    "                year = fileNumbers[3] #type = string\n",
    "                \n",
    "                # the following statement searches for the month in the filename\n",
    "                # if the find() function does not find the month, it returns '-1' and moves on to the next line.\n",
    "                # Thus the use of !=-1. \n",
    "                if fileString.find('mar') !=-1 or fileString.find('Mar') !=-1:\n",
    "                    quarter = 'Q1'\n",
    "                elif fileString.find('jun') !=-1 or fileString.find('Jun') !=-1:\n",
    "                    quarter = 'Q2'\n",
    "                elif fileString.find('sep') !=-1 or fileString.find('Sep') !=-1:\n",
    "                    quarter = 'Q3'\n",
    "                elif fileString.find('dec') !=-1 or fileString.find('Dec') !=-1:\n",
    "                    quarter = 'Q4'\n",
    "                df['time_period'] = year + ' ' + quarter\n",
    "\n",
    "                df['year'] = year\n",
    "\n",
    "                df['quarter'] = quarter\n",
    "                \n",
    "                # some of the columns in the files are not the same, so we fix them here\n",
    "                column = 'Quarterly change in Median Sales Price'\n",
    "                newColumns = {'Quarterly change in Median Sales Price':'Qtly change in Median',\n",
    "                             'Annual change in Median Sales Price':'Annual change in Median',\n",
    "                             'Quarterly change in Count':'Qtly change in Count'}\n",
    "        \n",
    "                if column in df.columns:\n",
    "                    df.rename(columns = newColumns, inplace= True)\n",
    "                \n",
    "                # finally, putting the DF into a list: frames\n",
    "                frames.extend([df])\n",
    "                \n",
    "               \n",
    "\n",
    "\n",
    "    # _____________________________________________________________________________________\n",
    "    # putting all the DFs (frames) together to get a master DF\n",
    "    masterDF = pd.concat(frames)\n",
    "    # General cleaning\n",
    "    rename_cols= {'Postcode':'postcode', \n",
    "             'Dwelling Type':'dwelling_type', \n",
    "             \"First Quartile Sales Price\\n$'000s\" : '25%_price',\n",
    "             \"Median Sales Price\\n$'000s\" : 'median_price', \n",
    "             \"Third Quartile Sales Price\\n'000s\" : '75%_price',\n",
    "             \"Mean Sales Price\\n$'000s\" : 'mean_price',\n",
    "             'Sales\\nNo.':'sales_no',\n",
    "             'Qtly change in Median':'Qdelta_median',\n",
    "             'Annual change in Median':'Adelta_median',\n",
    "             'Qtly change in Count':'Qdelta_count',\n",
    "             'Annual change in Count':'Adelta_count'}\n",
    "    \n",
    "    masterDF.rename(columns=rename_cols, inplace=True) #rename the columns for easier referencing\n",
    "\n",
    "\n",
    "\n",
    "    masterDF = masterDF.drop(columns=['25%_price', '75%_price'], axis=1) # dropping unwanted columns\n",
    "    \n",
    "    masterDF.loc[masterDF['sales_no'].isnull(), 'sales_no'] = 5.0 #imputing NAN values. 5 is median of 0 and 10 being the \n",
    "    # range for null values in the dataset. \n",
    "    \n",
    "    # fixing the NAN values in the median and mean columns \n",
    "    keys = list(masterDF['key'].unique())\n",
    "\n",
    "    for k in keys:\n",
    "    # Total\n",
    "    # First, median\n",
    "        k_impMedianTotal = masterDF.loc[(masterDF['median_price'].notna()) & \n",
    "                             (masterDF['dwelling_type']=='Total') &\n",
    "                             (masterDF['key']==k),\n",
    "                             'median_price'].median() # calculate imputer value \n",
    "\n",
    "        masterDF.loc[(masterDF['median_price'].isnull()) & \n",
    "                     (masterDF['dwelling_type']=='Total') &\n",
    "                     (masterDF['key']==k),\n",
    "                     'median_price']=k_impMedianTotal #impute\n",
    "\n",
    "    # mean\n",
    "        k_impMeanTotal = masterDF.loc[(masterDF['mean_price'].notna()) & \n",
    "                             (masterDF['dwelling_type']=='Total') &\n",
    "                             (masterDF['key']==k),\n",
    "                             'median_price'].median()\n",
    "\n",
    "        masterDF.loc[(masterDF['mean_price'].isnull()) & \n",
    "                 (masterDF['dwelling_type']=='Total') &\n",
    "                 (masterDF['key']==k),\n",
    "                 'mean_price']=k_impMeanTotal #impute\n",
    "#         print(k_impMeanTotal)\n",
    "#         print('')\n",
    "#         print(k)\n",
    "\n",
    "    # Strata\n",
    "    # First, median\n",
    "        k_impMedianStrata = masterDF.loc[(masterDF['median_price'].notna()) & \n",
    "                             (masterDF['dwelling_type']=='Strata') &\n",
    "                             (masterDF['key']==k),\n",
    "                             'median_price'].median()\n",
    "\n",
    "\n",
    "        masterDF.loc[(masterDF['median_price'].isnull()) & \n",
    "                     (masterDF['dwelling_type']=='Strata') &\n",
    "                     (masterDF['key']==k),\n",
    "                     'median_price']=k_impMedianStrata\n",
    "\n",
    "    # mean\n",
    "        k_impMeanStrata = masterDF.loc[(masterDF['mean_price'].notna()) & \n",
    "                             (masterDF['dwelling_type']=='Strata') &\n",
    "                             (masterDF['key']==k),\n",
    "                             'mean_price'].median()\n",
    "\n",
    "        masterDF.loc[(masterDF['mean_price'].isnull()) & \n",
    "                     (masterDF['dwelling_type']=='Strata') &\n",
    "                     (masterDF['key']==k),\n",
    "                     'mean_price']=k_impMeanStrata\n",
    "\n",
    "    # Non-Strata\n",
    "    # First, median\n",
    "        k_impMedianNonStrata = masterDF.loc[(masterDF['median_price'].notna()) & \n",
    "                             (masterDF['dwelling_type']=='Non Strata') &\n",
    "                             (masterDF['key']==k),\n",
    "                             'median_price'].median()\n",
    "\n",
    "        masterDF.loc[(masterDF['median_price'].isnull()) & \n",
    "                     (masterDF['dwelling_type']=='Non Strata') &\n",
    "                     (masterDF['key']==k),\n",
    "                     'median_price']=k_impMedianNonStrata\n",
    "\n",
    "    # mean\n",
    "        k_impMeanNonStrata = masterDF.loc[(masterDF['mean_price'].notna()) & \n",
    "                             (masterDF['dwelling_type']=='Non Strata') &\n",
    "                             (masterDF['key']==k),\n",
    "                             'mean_price'].median()\n",
    "\n",
    "        masterDF.loc[(masterDF['mean_price'].isnull()) & \n",
    "                     (masterDF['dwelling_type']=='Non Strata') &\n",
    "                     (masterDF['key']==k),\n",
    "                     'mean_price']=k_impMeanNonStrata\n",
    "        continue\n",
    "\n",
    "    masterDF.loc[masterDF['sales_no'] == 's', 'sales_no'] = 20.0 # Replace 's' with the median of \n",
    "    # 10 and 30 since there are quite a few\n",
    "\n",
    "    masterDF['sales_no'] = masterDF['sales_no'].astype(float) # Cast type as float\n",
    "\n",
    "    total = masterDF.loc[masterDF['dwelling_type']=='Total'] # Separate dwelling types\n",
    "    strata = masterDF.loc[masterDF['dwelling_type']=='Strata']\n",
    "    nstrata = masterDF.loc[masterDF['dwelling_type']=='Non Strata'] \n",
    "\n",
    "    return masterDF, total, strata, nstrata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803845a",
   "metadata": {},
   "source": [
    "Next we'll call the function and saving the dataframes as CSV. \n",
    "\n",
    "\n",
    "Below `old` denotes the files from **2017-2018** and `new` denotes the files from **2019-2021**. Sales files are cleaned in two batches (2017-2018, 2019-2021)because they take slightly different format in terms of the number of redundant rows before the actual table starts, and can be hard to be looped through in a single batch.\n",
    "\n",
    "Rent files have the same issue and are cleaned in the same fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "salesOld, salesOld_total, salesOld_strata, salesOld_nStrata  = salesCleanFn('Sales/2017_2018', 'windows')\n",
    "\n",
    "salesOld.to_csv('Files/sales_2017_2018')\n",
    "\n",
    "# salesOld_total.to_csv('Files/salesTotal_2017_2018')\n",
    "# salesOld_strata.to_csv('Files/salesStrata_2017_2018')\n",
    "# salesOld_nStrata.to_csv('Files/salesNstrata_2017_2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad1e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "salesNew, salesNew_total, salesNew_strata,  salesNew_nStrata =  salesCleanFn('Sales/2019_2021', 'windows')\n",
    "\n",
    "salesNew.to_csv('Files/sales_2019_2021')\n",
    "\n",
    "# salesNew_total.to_csv('Files/salesTotal_2019_2021')\n",
    "# salesNew_strata.to_csv('Files/salesStrata_2019_2021')\n",
    "# salesNew_nStrata.to_csv('Files/salesNstrata_2019_2021')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191aac1e",
   "metadata": {},
   "source": [
    "The resulting data files from the above process are:\n",
    "* sales_2017_2018.csv\n",
    "* sales_2019_2021.csv\n",
    "\n",
    "Both are saved in the outlier of the `Files` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed32166",
   "metadata": {},
   "source": [
    "### 1.2 Rent Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b29e4f",
   "metadata": {},
   "source": [
    "Define `rentCleanFn` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43acc7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rentCleanFn(dataFolderString,osString):\n",
    "        \n",
    "    #__________________________________________________________\n",
    "    # If operating system is windows\n",
    "    if osString == 'windows':\n",
    "        dataDir = \"Files\\\\\"\n",
    "        if dataFolderString[-1] != '\\\\':\n",
    "        #if the dataFolderString does not have a forward slash, add a forward slash to the string\n",
    "            fileNames = glob.glob(dataDir+dataFolderString+'\\\\'+'*.xlsx')\n",
    "        else:\n",
    "            fileNames = glob.glob(dataDir+dataFolderString+'*.xlsx')\n",
    "\n",
    "    #__________________________________________________________\n",
    "    # If operating system is mac / linux \n",
    "    else:\n",
    "        dataDir = \"Files/\"\n",
    "        if dataFolderString[-1] != '/':\n",
    "            fileNames = glob.glob(dataDir+dataFolderString+'/'+'*.xlsx')\n",
    "        else:\n",
    "            fileNames = glob.glob(dataDir+dataFolderString+'*.xlsx')\n",
    "#     print(fileNames)\n",
    "    \n",
    "    #__________________________________________________________\n",
    "    # Looping through the fileNames to read the excel sheets. \n",
    "    frames = []\n",
    "    masterDF = []\n",
    "    \n",
    "    for i, fileString in enumerate(fileNames):\n",
    "        for j in range(0,8):\n",
    "            df = []\n",
    "            df = pd.read_excel(fileString, sheet_name=\"Postcode\", na_values='-', header=j)\n",
    "\n",
    "            if df.columns[0] == 'Postcode': # ...if the 'header' parameter has the correct j value...\n",
    "\n",
    "                # adding additional columns\n",
    "                regex = re.compile(r'\\d+') #finds all numbers in string\n",
    "                fileNumbers = regex.findall(fileString)\n",
    "                \n",
    "                df['key'] = 'r'+fileNumbers[2] # fileNumbers type = string\n",
    "    \n",
    "                \n",
    "        \n",
    "                # some of the columns in the files are not the same, so we fix them here\n",
    "                column = 'Bedroom Numbers'\n",
    "                newColumns = {'Bedroom Numbers':'Number of Bedrooms'}\n",
    "        \n",
    "                if column in df.columns:\n",
    "                    df.rename(columns = newColumns, inplace= True)\n",
    "                \n",
    "                \n",
    "                \n",
    "                frames.extend([df])# putting the DF into a list, frames\n",
    "\n",
    "            \n",
    "              \n",
    "\n",
    "\n",
    "\n",
    "    masterDF = pd.concat(frames)\n",
    "    \n",
    "    # droppinig this column as we've confirmed there's an issue with the raw csv file. \n",
    "    if 'Unnamed: 10' in masterDF.columns:\n",
    "        masterDF=  masterDF.drop(columns='Unnamed: 10')\n",
    "\n",
    "    # Drop unwanted columns\n",
    "    masterDF = masterDF.drop(columns=['First Quartile Weekly Rent for New Bonds\\n$',\n",
    "                          'Third Quartile Weekly Rent for New Bonds\\n$'],\n",
    "                axis=1)\n",
    "    \n",
    "    # Rename columns\n",
    "    rename_cols= {'Postcode':'postcode',\n",
    "                  'Dwelling Types':'dwelling_type', \n",
    "                  'Number of Bedrooms':'bed_number',\n",
    "                  'Median Weekly Rent for New Bonds\\n$': 'median_rent_newb',\n",
    "                  'New Bonds Lodged\\nNo.' : 'new_bonds_no',\n",
    "                  'Total Bonds Held\\nNo.': 'total_bonds_no',\n",
    "                  'Quarterly change in Median Weekly Rent':'Qdelta_median_rent',\n",
    "                  'Annual change in Median Weekly Rent':'Adelta_median_rent',\n",
    "                  'Quarterly change in New Bonds Lodged':'Qdelta_new_bonds',\n",
    "                  'Annual change in New Bonds Lodged':'Adelta_new_bonds'}\n",
    "    \n",
    "    masterDF.rename(columns=rename_cols,inplace=True)\n",
    "\n",
    "    masterDF_ag = masterDF.loc[(masterDF['bed_number']=='Total') & (masterDF['dwelling_type']=='Total')]\n",
    "    masterDF_ag = masterDF_ag.drop(columns=['bed_number','dwelling_type'], axis=1)\n",
    "    \n",
    "    # Impute 's' in 'new_bonds_no' and 'total_bonds_no' with 20\n",
    "    masterDF_ag.loc[masterDF_ag['new_bonds_no']=='s','new_bonds_no'] = 20.0\n",
    "    masterDF_ag.loc[masterDF_ag['total_bonds_no']=='s', 'total_bonds_no'] = 20.0\n",
    "\n",
    "    # Impute na in 'new_bonds_no' and 'total_bonds_no' with 5\n",
    "    masterDF_ag.loc[masterDF_ag['new_bonds_no'].isnull(),'new_bonds_no'] = 5.0\n",
    "    masterDF_ag.loc[masterDF_ag['total_bonds_no'].isnull(), 'total_bonds_no'] = 5.0\n",
    "\n",
    "    # Cast both variables as float (was object)\n",
    "    masterDF_ag['new_bonds_no'] = masterDF_ag['new_bonds_no'].astype(float)\n",
    "    masterDF_ag['total_bonds_no'] = masterDF_ag['total_bonds_no'].astype(float)\n",
    "\n",
    "    # Impute na in 'median_rent' with median of the column\n",
    "    masterDF_ag['median_rent_newb'].fillna(masterDF_ag['median_rent_newb'].median(), inplace=True)\n",
    "    \n",
    "\n",
    "    # Set postcode as index\n",
    "    \n",
    "    masterDF_ag = masterDF_ag.set_index('postcode')\n",
    "    return masterDF_ag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268c7d5",
   "metadata": {},
   "source": [
    "Calling the function and saving the dataframes as CSV.\n",
    "\n",
    "As explained in previous section, rent files are cleaned in two batches as well and saved as two separate csv files for future use.\n",
    "* rent_2017_2018.csv\n",
    "* rent_2019_2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rentNew = rentCleanFn('Rent/2019_2021', 'windows')\n",
    "rentOld.to_csv('Files/rent_2017_2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rentOld  = rentCleanFn('Rent/2017_2018', 'windows')\n",
    "rentNew.to_csv('Files/rent_2019_2021')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204176a9",
   "metadata": {},
   "source": [
    "## STEP2. Merge Sales Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b0c25",
   "metadata": {},
   "source": [
    "Merge the cleaned `sales 2017-2018` and `sales 2019-2020` from step1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75675bcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Files/sales_2017_2018'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/22/9hdsbk0j58z1r2x79bx2zjh80000gn/T/ipykernel_63924/2021551677.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msales17_18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Files/sales_2017_2018\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m sales17_18 = pd.read_csv(sales17_18, usecols=['postcode', 'dwelling_type', 'median_price', 'mean_price',\n\u001b[0m\u001b[1;32m      5\u001b[0m                                                \u001b[0;34m'sales_no'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Qdelta_median'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Adelta_median'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Qdelta_count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                'Adelta_count', 'key', 'time_period', 'year', 'quarter'])\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Files/sales_2017_2018'"
     ]
    }
   ],
   "source": [
    "# Read in sales 2017-2018\n",
    "\n",
    "sales17_18 = \"Files/sales_2017_2018\"\n",
    "sales17_18 = pd.read_csv(sales17_18, usecols=['postcode', 'dwelling_type', 'median_price', 'mean_price',\n",
    "                                               'sales_no', 'Qdelta_median', 'Adelta_median', 'Qdelta_count',\n",
    "                                               'Adelta_count', 'key', 'time_period', 'year', 'quarter'])\n",
    "\n",
    "# Re-arrange column order\n",
    "cols = ['postcode', 'key', 'time_period', 'year', 'quarter', \n",
    "        'dwelling_type', 'median_price', 'mean_price','sales_no', \n",
    "        'Qdelta_median', 'Adelta_median', 'Qdelta_count','Adelta_count' ]\n",
    "\n",
    "sales17_18 = sales17_18[cols]\n",
    "sales17_18.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af48081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in sales 2019-2021\n",
    "\n",
    "sales19_21 = \"Files/sales_2019_2021\"\n",
    "sales19_21 = pd.read_csv(sales19_21, usecols=['postcode', 'dwelling_type', 'median_price', 'mean_price',\n",
    "                                               'sales_no', 'Qdelta_median', 'Adelta_median', 'Qdelta_count',\n",
    "                                               'Adelta_count', 'key', 'time_period', 'year', 'quarter'])\n",
    "# Re-arrange column order\n",
    "cols = ['postcode', 'key', 'time_period', 'year', 'quarter', \n",
    "        'dwelling_type', 'median_price', 'mean_price','sales_no', \n",
    "        'Qdelta_median', 'Adelta_median', 'Qdelta_count','Adelta_count' ]\n",
    "\n",
    "sales19_21 = sales19_21[cols]\n",
    "sales19_21.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79530435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two sales files \n",
    "sales_full = pd.concat([sales17_18, sales19_21])\n",
    "\n",
    "# Check if all quarters are present\n",
    "print(sales_full.groupby('time_period').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47750b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values\n",
    "sales_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf73ce",
   "metadata": {},
   "source": [
    "The delta variables will later be removed in the actual analysis but was kept here just in case they're of any use. Hence, we were not too concerned about the nulls in them and didn't do anything to clean them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0bf9ea",
   "metadata": {},
   "source": [
    "Save the complete sales dataset as .csv for later easier reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05989376",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_full.to_csv('Files/Cleaned/Sales_2017Q3_2021Q1_Clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab57c9",
   "metadata": {},
   "source": [
    "## STEP3. Merge Rent Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c867b",
   "metadata": {},
   "source": [
    "Merge the cleaned `rent 2017-2018` and `rent 2019-2020` from step1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec66545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read rent 2017-2018\n",
    "rent17_18 = \"Files/rent_2017_2018\"\n",
    "rent17_18 = pd.read_csv(rent17_18)\n",
    "\n",
    "# Read rent 2019-2021\n",
    "rent19_21 = \"Files/rent_2019_2021\"\n",
    "rent19_21 = pd.read_csv(rent19_21)\n",
    "\n",
    "# Concat both rent files\n",
    "rent_full = pd.concat([rent17_18, rent19_21])\n",
    "rent_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb567615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values\n",
    "rent_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553702a",
   "metadata": {},
   "source": [
    "Again, we don't really care about the null values in the delta variables as they'll later be dropped in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all quarters are presented\n",
    "rent_full.groupby('key').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61844ed",
   "metadata": {},
   "source": [
    "**NOTE:** rkey = skey-1 for the same quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map keys to time_periods so that can merge it with sales data later\n",
    "\n",
    "tp = ['2017 Q3', '2017 Q4', \n",
    "      '2018 Q1', '2018 Q2', '2018 Q3', '2018 Q4', \n",
    "      '2019 Q1', '2019 Q2', '2019 Q3', '2019 Q4', \n",
    "      '2020 Q1', '2020 Q2', '2020 Q3', '2020 Q4', \n",
    "      '2021 Q1']\n",
    "rkeys = ['r121','r122',\n",
    "         'r123','r124','r125','r126',\n",
    "         'r127','r128','r129','r130',\n",
    "         'r131','r132','r133','r134',\n",
    "         'r135']\n",
    "\n",
    "rent_full['time_period'] = np.nan\n",
    "\n",
    "for i in list(range(0,15)):\n",
    "    rent_full.loc[rent_full['key']==rkeys[i], 'time_period']=tp[i]\n",
    "    \n",
    "rent_full.groupby('time_period').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update column name of 'key'\n",
    "rent_full = rent_full.rename(columns={'key':'rkey'})\n",
    "\n",
    "\n",
    "# Change columns order\n",
    "cols = ['postcode', 'rkey', 'time_period', 'median_rent_newb', 'new_bonds_no', 'total_bonds_no',\n",
    "       'Qdelta_median_rent', 'Qdelta_new_bonds', 'Adelta_median_rent',\n",
    "       'Adelta_new_bonds']\n",
    "rent_full = rent_full[cols]\n",
    "\n",
    "rent_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d031ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full rent data into csv\n",
    "rent_full.to_csv('Files/Cleaned/Rent_2017Q4_2021Q2_Clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40431515",
   "metadata": {},
   "source": [
    "## STEP4. Create Master Stacked Sales & Rent File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4570226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the name of 'key' column in the sales files from 'key' to 'skey' \n",
    "# to differentiate from 'rkey' in rent\n",
    "sales_full = sales_full.rename(columns={'key':'skey'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the full sales data DF and the full rent data DF\n",
    "\n",
    "sales_rent_full = pd.merge(sales_full, rent_full, how='left',\n",
    "                           left_on=['postcode','time_period'],\n",
    "                           right_on=['postcode', 'time_period'])\n",
    "\n",
    "sales_rent_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6fa69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales_rent_full.shape)\n",
    "print(sales_rent_full.groupby('time_period').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99402cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales_rent_full.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14337db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the 6 postcodes that are null in rent:\n",
    "sales_rent_full.loc[sales_rent_full['rkey'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rent_full.to_csv('Files/Cleaned/Master_Sales_Rent_2017Q4_2021Q1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c6ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rent_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34254c4e",
   "metadata": {},
   "source": [
    "##  STEP5: Create Master Unstacked Sales & Rent File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d5001b",
   "metadata": {},
   "source": [
    "We've decided to only include housing data from the previous year or the four quarters (i.e. Q1 2020 to Q4 2020) as predictor variables for the Q1 2021 prices. Hence, in the unstacked data file we're only keeping 5 quarters (incl. Q1 2021). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset that only contains 5 Quarters of data\n",
    "subset = sales_rent_full.loc[sales_rent_full['time_period'].isin(['2020 Q1','2020 Q2','2020 Q3','2020 Q4','2021 Q1'])]\n",
    "\n",
    "# Get some of the (potentially) unnecessary variables\n",
    "subset = subset.drop(columns=['Qdelta_median','Adelta_median','Qdelta_count','Adelta_count',\n",
    "                              'Qdelta_median_rent', 'Adelta_median_rent','Qdelta_new_bonds','Adelta_new_bonds'],\n",
    "                     axis=1)\n",
    "\n",
    "# And only keep 'Total' dwelling type (i.e. get rid of Strata and Non-strata)\n",
    "subset_total = subset.loc[subset['dwelling_type'] == 'Total']\n",
    "\n",
    "print(subset_total.groupby('time_period').size(),'\\n')\n",
    "print(subset_total.groupby('dwelling_type').size(),'\\n')\n",
    "print(subset_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4857cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_total.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e963aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of postcode in Q4 2020:\",\n",
    "      subset_total.loc[subset_total['time_period']=='2020 Q4', 'postcode'].nunique())\n",
    "print(\"number of postcode in Q1 2021:\",\n",
    "      subset_total.loc[subset_total['time_period']=='2021 Q1', 'postcode'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3bec0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pivot = subset_total.pivot_table(index='postcode', columns='time_period', values=\n",
    "                                 ['median_price', 'mean_price', 'sales_no', \n",
    "                                  'median_rent_newb','new_bonds_no', 'total_bonds_no']).round(2)\n",
    "\n",
    "pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ca960",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot.columns = [' '.join(col) for col in pivot.columns]\n",
    "pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pivot.shape)\n",
    "print(pivot.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202b80c",
   "metadata": {},
   "source": [
    "Because each quarterly table has different numbers of postcodes and they're not an exact match from quarter to quarter, there will be additional null values generated after pivoting simply due to certain postcodes having data in some quarters but not in others. To serve sample size, we're going to impute the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ca3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_fillna = pivot.fillna(pivot.median()).round(1)\n",
    "pivot_fillna.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_fillna.reset_index(inplace=True)\n",
    "pivot_fillna.to_csv('Files/Cleaned/Pivot_Sales_Rent_5Quarters_Imputed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef564217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pivot = subset_total.pivot_table(index='postcode', columns='time_period', values=\n",
    "                                 ['median_price', 'mean_price', 'sales_no', \n",
    "                                  'median_rent_newb','new_bonds_no', 'total_bonds_no']).round(2)\n",
    "\n",
    "pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ca960",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot.columns = [' '.join(col) for col in pivot.columns]\n",
    "pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pivot.shape)\n",
    "print(pivot.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ca3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_fillna = pivot.fillna(pivot.median()).round(1)\n",
    "pivot_fillna.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_fillna.reset_index(inplace=True)\n",
    "pivot_fillna.to_csv('Files/Cleaned/Pivot_Sales_Rent_5Quarters_Imputed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265.1875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
